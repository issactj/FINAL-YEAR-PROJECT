{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.4.3)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\dell\\appdata\\roaming\\python\\python37\\site-packages (from keras) (1.17.4)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from keras) (1.5.2)\n",
      "Requirement already satisfied: h5py in c:\\users\\dell\\anaconda3\\lib\\site-packages (from keras) (3.3.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\dell\\anaconda3\\lib\\site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: cached-property; python_version < \"3.8\" in c:\\users\\dell\\anaconda3\\lib\\site-packages (from h5py->keras) (1.5.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.5.0-cp37-cp37m-win_amd64.whl (422.6 MB)\n",
      "Processing c:\\users\\dell\\appdata\\local\\pip\\cache\\wheels\\3f\\e3\\ec\\8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\\termcolor-1.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (3.17.3)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (0.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\dell\\AppData\\Roaming\\Python\\Python37\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script markdown_py.exe is installed in 'C:\\Users\\dell\\AppData\\Roaming\\Python\\Python37\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\dell\\AppData\\Roaming\\Python\\Python37\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts estimator_ckpt_converter.exe, import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\\Users\\dell\\AppData\\Roaming\\Python\\Python37\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-pasta~=0.2\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting numpy~=1.19.2\n",
      "  Using cached numpy-1.19.5-cp37-cp37m-win_amd64.whl (13.2 MB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (0.35.1)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting gast==0.4.0\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Processing c:\\users\\dell\\appdata\\local\\pip\\cache\\wheels\\62\\76\\4c\\aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\\wrapt-1.12.1-py3-none-any.whl\n",
      "Collecting keras-nightly~=2.5.0.dev\n",
      "  Using cached keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Using cached h5py-3.1.0-cp37-cp37m-win_amd64.whl (2.7 MB)\n",
      "Collecting grpcio~=1.34.0\n",
      "  Using cached grpcio-1.34.1-cp37-cp37m-win_amd64.whl (2.9 MB)\n",
      "Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
      "  Using cached tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting tensorboard~=2.5\n",
      "  Using cached tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: cached-property; python_version < \"3.8\" in c:\\users\\dell\\anaconda3\\lib\\site-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (1.28.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (50.3.1.post20201107)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (0.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (2.24.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\dell\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2020.11.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.0)\n",
      "Installing collected packages: termcolor, google-pasta, numpy, opt-einsum, keras-preprocessing, gast, wrapt, keras-nightly, flatbuffers, h5py, grpcio, tensorflow-estimator, typing-extensions, tensorboard-plugin-wit, markdown, tensorboard-data-server, tensorboard, astunparse, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.17.4\n",
      "    Uninstalling numpy-1.17.4:\n",
      "      Successfully uninstalled numpy-1.17.4\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-1.12 gast-0.4.0 google-pasta-0.2.0 grpcio-1.34.1 h5py-3.1.0 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 opt-einsum-3.3.0 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.5.0 tensorflow-estimator-2.5.0 termcolor-1.1.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydot_ng\n",
      "  Downloading pydot_ng-2.0.0-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pydot_ng) (2.4.7)\n",
      "Installing collected packages: pydot-ng\n",
      "Successfully installed pydot-ng-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pydot_ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphviz\n",
      "  Downloading graphviz-0.16-py2.py3-none-any.whl (19 kB)\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.16\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydot\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pydot) (2.4.7)\n",
      "Installing collected packages: pydot\n",
      "Successfully installed pydot-1.4.2\n"
     ]
    }
   ],
   "source": [
    "pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Import all the important required Deep Learning Libraries to train the emotions model.\n",
    "    \n",
    "    Keras is an Application Programming Interface (API) which can run on top of tensorflow.\n",
    "    \n",
    "    tensorflow will be the main deep learning module we will use to build our deep learning model.\n",
    "    \n",
    "    The ImageDataGenerator is used for Data augmentation where the model can see more copies of \n",
    "        the model. Data Augmentation is used for creating replications of the original images \n",
    "        and using those transformations in each epoch.\n",
    "        \n",
    "    The layers for training which will be used are as follows:\n",
    "    1. Input = The input layer which we pass the input shape.\n",
    "    2. Conv2D = The Convoluional layer combined with Input to provide a output of tensors\n",
    "    3. Maxpool2D = Downsampling the Data from the convolutional layer.\n",
    "    4. Batch normalization = It is a technique for training very deep neural networks that standardizes \n",
    "       the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning \n",
    "       process and dramatically reducing the number of training epochs required to train deep networks.\n",
    "    5. Dropout = Dropout is a technique where randomly selected neurons are ignored during training. \n",
    "                 They are “dropped-out” randomly and this prevents over-fitting.\n",
    "    6. Dense = Fully Connected layers.\n",
    "    7. Flatten = Flatten the entire structure to a 1-D array.\n",
    "    \n",
    "    The Models can be built in a model like structure as shown in this particular model or can be built \n",
    "        in a sequential manner.\n",
    "        \n",
    "    Use of l2 regularization for fine tuning.\n",
    "    \n",
    "    The optimizer used will be Adam as it is performs better than the other optimizers on this model.\n",
    "    \n",
    "    Numpy for numerical array like operations.\n",
    "    \n",
    "    pydot_ng and graphviz are used for making plots.\n",
    "    \n",
    "    We are also importing the os module to make it compatible with the windows environment. \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, BatchNormalization, Dense, Flatten, Activation\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pydot_ng\n",
    "import graphviz\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    num_classes = Defines the number of classes we have to predict which are namely \n",
    "    Angry, Fear, Happy, Neutral, Surprise, Neutral and Disgust.\n",
    "    \n",
    "    From the exploratory Data Analysis we know that The Dimensions of the image are:\n",
    "    Image Height = 48 pixels\n",
    "    Image Width = 48 pixels\n",
    "    Number of channels = 1 because it is a grayscale image.\n",
    "    \n",
    "    We will consider a batch size for the training of the image augmentation.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "num_classes = 7\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "Img_height = 48\n",
    "Img_width = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Store the csv file in data and then print the first 5 rows.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "data = pd.read_csv('C:/Users/dell/Desktop/PROJECT DATA/fer2013.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Reference: https://github.com/oarriaga/face_classification\n",
    "    We will convert the pixels to list in this method. \n",
    "    We split the data by spaces and then take them as arrays and \n",
    "    reshape into 48, 48 shape. Then we expand the dimensions \n",
    "    and then convert the labels to categorical matrix.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "pixels = data['pixels'].tolist() \n",
    "faces = []\n",
    "\n",
    "for pixel_sequence in pixels:\n",
    "    face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
    "    face = np.asarray(face).reshape(Img_height, Img_width) \n",
    "    faces.append(face.astype('float32'))\n",
    "\n",
    "faces = np.asarray(faces)\n",
    "faces = np.expand_dims(faces, -1)\n",
    "\n",
    "emotions = pd.get_dummies(data['emotion']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(faces, emotions, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    We will be using a sequential type architecture for our model.\n",
    "    Our Sequential model will have a total of 5 blocks i.e. three\n",
    "    convolutional blocks, one fully connected layer and one output\n",
    "    layer.\n",
    "    \n",
    "    We will have 3 Convolutional Blocks with filters of increasing size\n",
    "    as 32, 64 and 128 respectively. The kernel_size will be (3,3) and\n",
    "    the kernel_initializer will be he_normal. We can also use a \n",
    "    kernel_regularizer with l2 normalization. Our Preferred choice of \n",
    "    activation is relu because it usually performs better on most datasets.\n",
    "    The Input shape will be the same as the size of each of our train and\n",
    "    validation images.\n",
    "    The BatchNormalization layer Batch normalization is a technique for improving \n",
    "    the speed, performance, and stability of artificial neural networks.\n",
    "    and Maxpooling is used to Downsample the data. \n",
    "    The Dropout layer is used for prevention of overfitting.\n",
    "    \n",
    "    The fully connected block consists of a Dense layer of 64 filters\n",
    "    and a batch normalization followed by a dropout layer. Before passing\n",
    "    through the Dense layer the data is flattened to match the dimensions.\n",
    "    \n",
    "    Finally the output layer consists of a Dense layer with a softmax\n",
    "    activation to give probabilites according to the num_classes which\n",
    "    represents the number of predictions to be made. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Block-1: The First Convolutional Block\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', \n",
    "                 kernel_initializer='he_normal',\n",
    "                 activation=\"relu\", \n",
    "                 input_shape=(Img_height, Img_width, 1), \n",
    "                 name=\"Conv1\"))\n",
    "\n",
    "model.add(BatchNormalization(name=\"Batch_Norm1\"))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', \n",
    "                 kernel_initializer='he_normal', \n",
    "                 activation=\"relu\", name=\"Conv2\"))\n",
    "\n",
    "model.add(BatchNormalization(name=\"Batch_Norm2\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), name=\"Maxpool1\"))\n",
    "model.add(Dropout(0.5, name=\"Dropout1\"))\n",
    "\n",
    "# Block-2: The Second Convolutional Block\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', \n",
    "                 kernel_initializer='he_normal',\n",
    "                 activation=\"relu\", name=\"Conv3\"))\n",
    "\n",
    "model.add(BatchNormalization(name=\"Batch_Norm3\"))\n",
    "\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),padding='same',\n",
    "                 kernel_initializer='he_normal', \n",
    "                 activation=\"relu\", name=\"Conv4\"))\n",
    "\n",
    "model.add(BatchNormalization(name=\"Batch_Norm4\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), name=\"Maxpool2\"))\n",
    "model.add(Dropout(0.5, name=\"Dropout2\"))\n",
    "\n",
    "# Block-3: The Third Convolutional Block\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', \n",
    "                 kernel_initializer='he_normal', \n",
    "                 activation=\"relu\", name=\"Conv5\"))\n",
    "\n",
    "model.add(BatchNormalization(name=\"Batch_Norm5\"))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', \n",
    "                 kernel_initializer='he_normal',\n",
    "                 activation=\"relu\", name=\"Conv6\"))\n",
    "\n",
    "model.add(BatchNormalization(name=\"Batch_Norm6\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), name=\"Maxpool3\"))\n",
    "model.add(Dropout(0.5, name=\"Dropout3\"))\n",
    "\n",
    "# Block-4: The Fully Connected Block\n",
    "\n",
    "model.add(Flatten(name=\"Flatten\"))\n",
    "model.add(Dense(64, activation=\"relu\", kernel_initializer='he_normal', name=\"Dense\"))\n",
    "model.add(BatchNormalization(name=\"Batch_Norm7\"))\n",
    "model.add(Dropout(0.5, name=\"Dropout4\"))\n",
    "\n",
    "# Block-5: The Output Block\n",
    "\n",
    "model.add(Dense(num_classes, activation=\"softmax\", kernel_initializer='he_normal', name = \"Output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1 (Conv2D)               (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "Batch_Norm1 (BatchNormalizat (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "Batch_Norm2 (BatchNormalizat (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "Maxpool1 (MaxPooling2D)      (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "Dropout1 (Dropout)           (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "Conv3 (Conv2D)               (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "Batch_Norm3 (BatchNormalizat (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "Conv4 (Conv2D)               (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "Batch_Norm4 (BatchNormalizat (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "Maxpool2 (MaxPooling2D)      (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "Dropout2 (Dropout)           (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv5 (Conv2D)               (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "Batch_Norm5 (BatchNormalizat (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "Conv6 (Conv2D)               (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "Batch_Norm6 (BatchNormalizat (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "Maxpool3 (MaxPooling2D)      (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "Dropout3 (Dropout)           (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "Flatten (Flatten)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 64)                294976    \n",
      "_________________________________________________________________\n",
      "Batch_Norm7 (BatchNormalizat (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "Dropout4 (Dropout)           (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 583,911\n",
      "Trainable params: 582,887\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils import np_utils\n",
    "\n",
    "keras.utils.plot_model(model, to_file='model3.png', show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7),\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='logs3')\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_accuracy', factor=0.9, patience=3, verbose=1)\n",
    "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=8, verbose=1, mode='auto')\n",
    "checkpointer = ModelCheckpoint(\"emotions3.h5\", monitor='val_accuracy', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "455/455 [==============================] - 260s 563ms/step - loss: 2.0942 - accuracy: 0.2444 - val_loss: 1.7046 - val_accuracy: 0.3123\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.31234, saving model to emotions3.h5\n",
      "Epoch 2/100\n",
      "455/455 [==============================] - 278s 610ms/step - loss: 1.6894 - accuracy: 0.3624 - val_loss: 1.4580 - val_accuracy: 0.4355\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.31234 to 0.43550, saving model to emotions3.h5\n",
      "Epoch 3/100\n",
      "455/455 [==============================] - 265s 582ms/step - loss: 1.4992 - accuracy: 0.4271 - val_loss: 1.3255 - val_accuracy: 0.4826\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.43550 to 0.48259, saving model to emotions3.h5\n",
      "Epoch 4/100\n",
      "455/455 [==============================] - 279s 613ms/step - loss: 1.3965 - accuracy: 0.4660 - val_loss: 1.2676 - val_accuracy: 0.5093\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.48259 to 0.50933, saving model to emotions3.h5\n",
      "Epoch 5/100\n",
      "455/455 [==============================] - 262s 576ms/step - loss: 1.3357 - accuracy: 0.4937 - val_loss: 1.2257 - val_accuracy: 0.5183\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.50933 to 0.51825, saving model to emotions3.h5\n",
      "Epoch 6/100\n",
      "455/455 [==============================] - 255s 561ms/step - loss: 1.2853 - accuracy: 0.5129 - val_loss: 1.1861 - val_accuracy: 0.5386\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.51825 to 0.53859, saving model to emotions3.h5\n",
      "Epoch 7/100\n",
      "455/455 [==============================] - 254s 559ms/step - loss: 1.2592 - accuracy: 0.5210 - val_loss: 1.1484 - val_accuracy: 0.5548\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.53859 to 0.55475, saving model to emotions3.h5\n",
      "Epoch 8/100\n",
      "455/455 [==============================] - 255s 561ms/step - loss: 1.2303 - accuracy: 0.5320 - val_loss: 1.1244 - val_accuracy: 0.5631\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.55475 to 0.56311, saving model to emotions3.h5\n",
      "Epoch 9/100\n",
      "455/455 [==============================] - 255s 560ms/step - loss: 1.2303 - accuracy: 0.5308 - val_loss: 1.1277 - val_accuracy: 0.5609\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.56311\n",
      "Epoch 10/100\n",
      "455/455 [==============================] - 258s 566ms/step - loss: 1.1824 - accuracy: 0.5534 - val_loss: 1.1098 - val_accuracy: 0.5737\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.56311 to 0.57370, saving model to emotions3.h5\n",
      "Epoch 11/100\n",
      "455/455 [==============================] - 259s 570ms/step - loss: 1.1677 - accuracy: 0.5580 - val_loss: 1.1271 - val_accuracy: 0.5726\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.57370\n",
      "Epoch 12/100\n",
      "455/455 [==============================] - 257s 565ms/step - loss: 1.1365 - accuracy: 0.5734 - val_loss: 1.2896 - val_accuracy: 0.5520\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.57370\n",
      "Epoch 13/100\n",
      "455/455 [==============================] - 257s 564ms/step - loss: 1.1191 - accuracy: 0.5780 - val_loss: 1.0680 - val_accuracy: 0.5957\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.57370 to 0.59571, saving model to emotions3.h5\n",
      "Epoch 14/100\n",
      "455/455 [==============================] - 257s 564ms/step - loss: 1.1034 - accuracy: 0.5840 - val_loss: 1.0647 - val_accuracy: 0.5865\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.59571\n",
      "Epoch 15/100\n",
      "455/455 [==============================] - 255s 560ms/step - loss: 1.0865 - accuracy: 0.5937 - val_loss: 1.0237 - val_accuracy: 0.6074\n",
      "\n",
      "Epoch 00015: val_accuracy improved from 0.59571 to 0.60741, saving model to emotions3.h5\n",
      "Epoch 16/100\n",
      "455/455 [==============================] - 253s 556ms/step - loss: 1.0762 - accuracy: 0.5988 - val_loss: 1.0218 - val_accuracy: 0.6116\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.60741 to 0.61159, saving model to emotions3.h5\n",
      "Epoch 17/100\n",
      "455/455 [==============================] - 253s 556ms/step - loss: 1.0620 - accuracy: 0.6029 - val_loss: 1.0218 - val_accuracy: 0.6110\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.61159\n",
      "Epoch 18/100\n",
      "455/455 [==============================] - 253s 556ms/step - loss: 1.0374 - accuracy: 0.6127 - val_loss: 1.0272 - val_accuracy: 0.6119\n",
      "\n",
      "Epoch 00018: val_accuracy improved from 0.61159 to 0.61187, saving model to emotions3.h5\n",
      "Epoch 19/100\n",
      "455/455 [==============================] - 253s 556ms/step - loss: 1.0367 - accuracy: 0.6122 - val_loss: 1.0210 - val_accuracy: 0.6141\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.61187 to 0.61410, saving model to emotions3.h5\n",
      "Epoch 20/100\n",
      "455/455 [==============================] - 256s 564ms/step - loss: 1.0218 - accuracy: 0.6177 - val_loss: 1.0067 - val_accuracy: 0.6208\n",
      "\n",
      "Epoch 00020: val_accuracy improved from 0.61410 to 0.62079, saving model to emotions3.h5\n",
      "Epoch 21/100\n",
      "455/455 [==============================] - 254s 558ms/step - loss: 1.0063 - accuracy: 0.6241 - val_loss: 0.9977 - val_accuracy: 0.6255\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.62079 to 0.62552, saving model to emotions3.h5\n",
      "Epoch 22/100\n",
      "455/455 [==============================] - 254s 557ms/step - loss: 0.9984 - accuracy: 0.6297 - val_loss: 1.0510 - val_accuracy: 0.6138\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.62552\n",
      "Epoch 23/100\n",
      "455/455 [==============================] - 253s 557ms/step - loss: 0.9873 - accuracy: 0.6311 - val_loss: 0.9912 - val_accuracy: 0.6280\n",
      "\n",
      "Epoch 00023: val_accuracy improved from 0.62552 to 0.62803, saving model to emotions3.h5\n",
      "Epoch 24/100\n",
      "455/455 [==============================] - 253s 557ms/step - loss: 0.9756 - accuracy: 0.6361 - val_loss: 0.9956 - val_accuracy: 0.6261\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.62803\n",
      "Epoch 25/100\n",
      "455/455 [==============================] - 253s 556ms/step - loss: 0.9629 - accuracy: 0.6399 - val_loss: 0.9906 - val_accuracy: 0.6325\n",
      "\n",
      "Epoch 00025: val_accuracy improved from 0.62803 to 0.63249, saving model to emotions3.h5\n",
      "Epoch 26/100\n",
      "455/455 [==============================] - 251s 553ms/step - loss: 0.9549 - accuracy: 0.6436 - val_loss: 0.9891 - val_accuracy: 0.6339\n",
      "\n",
      "Epoch 00026: val_accuracy improved from 0.63249 to 0.63388, saving model to emotions3.h5\n",
      "Epoch 27/100\n",
      "455/455 [==============================] - 252s 554ms/step - loss: 0.9511 - accuracy: 0.6447 - val_loss: 1.0001 - val_accuracy: 0.6311\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.63388\n",
      "Epoch 28/100\n",
      "455/455 [==============================] - 254s 558ms/step - loss: 0.9391 - accuracy: 0.6496 - val_loss: 1.0027 - val_accuracy: 0.6342\n",
      "\n",
      "Epoch 00028: val_accuracy improved from 0.63388 to 0.63416, saving model to emotions3.h5\n",
      "Epoch 29/100\n",
      "455/455 [==============================] - 251s 552ms/step - loss: 0.9237 - accuracy: 0.6559 - val_loss: 0.9959 - val_accuracy: 0.6453\n",
      "\n",
      "Epoch 00029: val_accuracy improved from 0.63416 to 0.64531, saving model to emotions3.h5\n",
      "Epoch 30/100\n",
      "455/455 [==============================] - 252s 554ms/step - loss: 0.9209 - accuracy: 0.6612 - val_loss: 0.9679 - val_accuracy: 0.6417\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.64531\n",
      "Epoch 31/100\n",
      "455/455 [==============================] - 252s 555ms/step - loss: 0.9015 - accuracy: 0.6608 - val_loss: 0.9735 - val_accuracy: 0.6436\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.64531\n",
      "Epoch 32/100\n",
      "455/455 [==============================] - 252s 554ms/step - loss: 0.9015 - accuracy: 0.6663 - val_loss: 0.9977 - val_accuracy: 0.6406\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.64531\n",
      "Epoch 33/100\n",
      "455/455 [==============================] - 253s 557ms/step - loss: 0.8891 - accuracy: 0.6716 - val_loss: 0.9959 - val_accuracy: 0.6403\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.64531\n",
      "Epoch 34/100\n",
      "455/455 [==============================] - 253s 556ms/step - loss: 0.8782 - accuracy: 0.6745 - val_loss: 0.9783 - val_accuracy: 0.6503\n",
      "\n",
      "Epoch 00034: val_accuracy improved from 0.64531 to 0.65032, saving model to emotions3.h5\n",
      "Epoch 35/100\n",
      "455/455 [==============================] - 255s 561ms/step - loss: 0.8713 - accuracy: 0.6767 - val_loss: 0.9859 - val_accuracy: 0.6414\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.65032\n",
      "Epoch 36/100\n",
      "455/455 [==============================] - 256s 562ms/step - loss: 0.8653 - accuracy: 0.6777 - val_loss: 1.0104 - val_accuracy: 0.6406\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.65032\n",
      "Epoch 37/100\n",
      "455/455 [==============================] - 257s 564ms/step - loss: 0.8550 - accuracy: 0.6808 - val_loss: 0.9903 - val_accuracy: 0.6456\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0008100000384729356.\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.65032\n",
      "Epoch 38/100\n",
      "455/455 [==============================] - 256s 563ms/step - loss: 0.8434 - accuracy: 0.6877 - val_loss: 0.9954 - val_accuracy: 0.6470\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.65032\n",
      "Epoch 39/100\n",
      "455/455 [==============================] - 262s 577ms/step - loss: 0.8457 - accuracy: 0.6859 - val_loss: 0.9689 - val_accuracy: 0.6567\n",
      "\n",
      "Epoch 00039: val_accuracy improved from 0.65032 to 0.65673, saving model to emotions3.h5\n",
      "Epoch 40/100\n",
      "455/455 [==============================] - 260s 571ms/step - loss: 0.8322 - accuracy: 0.6939 - val_loss: 0.9816 - val_accuracy: 0.6514\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.65673\n",
      "Epoch 41/100\n",
      "455/455 [==============================] - 257s 566ms/step - loss: 0.8352 - accuracy: 0.6882 - val_loss: 0.9902 - val_accuracy: 0.6484\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.65673\n",
      "Epoch 42/100\n",
      "455/455 [==============================] - 256s 562ms/step - loss: 0.8320 - accuracy: 0.6909 - val_loss: 0.9782 - val_accuracy: 0.6539\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0007290000503417104.\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.65673\n",
      "Epoch 43/100\n",
      "455/455 [==============================] - 256s 562ms/step - loss: 0.8087 - accuracy: 0.7000 - val_loss: 0.9836 - val_accuracy: 0.6545\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.65673\n",
      "Epoch 44/100\n",
      "455/455 [==============================] - 256s 562ms/step - loss: 0.8021 - accuracy: 0.7032 - val_loss: 0.9776 - val_accuracy: 0.6584\n",
      "\n",
      "Epoch 00044: val_accuracy improved from 0.65673 to 0.65840, saving model to emotions3.h5\n",
      "Epoch 45/100\n",
      "455/455 [==============================] - 255s 561ms/step - loss: 0.7986 - accuracy: 0.7037 - val_loss: 0.9962 - val_accuracy: 0.6525\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.65840\n",
      "Epoch 46/100\n",
      "455/455 [==============================] - 256s 562ms/step - loss: 0.7906 - accuracy: 0.7067 - val_loss: 0.9961 - val_accuracy: 0.6509\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.65840\n",
      "Epoch 47/100\n",
      "455/455 [==============================] - 256s 562ms/step - loss: 0.7855 - accuracy: 0.7070 - val_loss: 0.9782 - val_accuracy: 0.6592\n",
      "\n",
      "Epoch 00047: val_accuracy improved from 0.65840 to 0.65924, saving model to emotions3.h5\n",
      "Epoch 48/100\n",
      "455/455 [==============================] - 257s 564ms/step - loss: 0.7817 - accuracy: 0.7110 - val_loss: 0.9681 - val_accuracy: 0.6629\n",
      "\n",
      "Epoch 00048: val_accuracy improved from 0.65924 to 0.66286, saving model to emotions3.h5\n",
      "Epoch 49/100\n",
      "455/455 [==============================] - 255s 562ms/step - loss: 0.7803 - accuracy: 0.7095 - val_loss: 0.9715 - val_accuracy: 0.6570\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.66286\n",
      "Epoch 50/100\n",
      "455/455 [==============================] - 257s 564ms/step - loss: 0.7697 - accuracy: 0.7142 - val_loss: 0.9775 - val_accuracy: 0.6587\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.66286\n",
      "Epoch 51/100\n",
      "455/455 [==============================] - 255s 561ms/step - loss: 0.7705 - accuracy: 0.7113 - val_loss: 0.9969 - val_accuracy: 0.6567\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0006561000715009868.\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.66286\n",
      "Epoch 52/100\n",
      "455/455 [==============================] - 253s 557ms/step - loss: 0.7516 - accuracy: 0.7193 - val_loss: 0.9766 - val_accuracy: 0.6623\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.66286\n",
      "Epoch 53/100\n",
      "455/455 [==============================] - 251s 552ms/step - loss: 0.7580 - accuracy: 0.7191 - val_loss: 0.9769 - val_accuracy: 0.6629\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.66286\n",
      "Epoch 54/100\n",
      "455/455 [==============================] - 254s 559ms/step - loss: 0.7569 - accuracy: 0.7218 - val_loss: 0.9976 - val_accuracy: 0.6592\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.0005904900433961303.\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.66286\n",
      "Epoch 55/100\n",
      "455/455 [==============================] - 252s 553ms/step - loss: 0.7355 - accuracy: 0.7279 - val_loss: 0.9733 - val_accuracy: 0.6623\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.66286\n",
      "Epoch 56/100\n",
      "455/455 [==============================] - 254s 558ms/step - loss: 0.7495 - accuracy: 0.7238 - val_loss: 0.9796 - val_accuracy: 0.6576\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.66286\n",
      "Epoch 00056: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ba298dfc48>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.array(X_train), np.array(y_train),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(np.array(X_test), np.array(y_test)),\n",
    "          shuffle=True,\n",
    "          callbacks=[lr_reducer, tensorboard, early_stopper, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
